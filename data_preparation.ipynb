{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78a0e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n",
      "User:  nchitiwong\n",
      "Database:  postgresql://nchitiwong:secret@ads1.datasci.vt.edu:5432/ads_db5\n"
     ]
    }
   ],
   "source": [
    "%run \"./env_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdb3ae",
   "metadata": {},
   "source": [
    "# Data Preparation for Stress Testing\n",
    "\n",
    "This notebook prepares the data for ARIMAX modeling:\n",
    "1. Load and clean historical data\n",
    "2. Handle missing values and outliers\n",
    "3. Create time series structures\n",
    "4. Prepare features for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d391c",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b8161a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sselte7wp0l",
   "metadata": {},
   "source": [
    "## 1.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "itehua13h9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Constants\n",
    "CONFIG = {\n",
    "    'tables': {\n",
    "        'economic': 'nchitiwong.historical_economic_data',\n",
    "        'losses': 'nchitiwong.historical_loan_losses',\n",
    "        'scenarios': 'nchitiwong.stress_test_scenarios',\n",
    "        'capital': 'nchitiwong.bank_capital_structure',\n",
    "        'portfolio': 'nchitiwong.bank_portfolio',\n",
    "        'proj_losses': 'nchitiwong.scenario_projected_losses'\n",
    "    },\n",
    "    'portfolio_map': {\n",
    "        1: 'Residential_Mortgages',\n",
    "        2: 'Commercial_Loans',\n",
    "        3: 'Credit_Cards',\n",
    "        4: 'Securities_Portfolio'\n",
    "    },\n",
    "    'economic_vars': [\n",
    "        'gdp_growth',\n",
    "        'unemployment_rate', \n",
    "        'housing_price_change',\n",
    "        'fed_funds_rate',\n",
    "        'treasury_10y_rate',\n",
    "        'credit_spread_bps',\n",
    "        'stock_market_decline',\n",
    "        'vix_level',\n",
    "        'corp_bond_spread_bps'\n",
    "    ],\n",
    "    'expected_months': 120,\n",
    "    'date_column': 'cal_date'\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zllytse0vm",
   "metadata": {},
   "source": [
    "## 1.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ytkce4l3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_table(table_name, order_by=None):\n",
    "    \"\"\"\n",
    "    Load data from database with error handling.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Full table name (e.g., 'jackn.historical_economic_data')\n",
    "        order_by: Optional ORDER BY clause (e.g., 'cal_date')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sql = f\"SELECT * FROM {table_name}\"\n",
    "        if order_by:\n",
    "            sql += f\" ORDER BY {order_by}\"\n",
    "        sql += \";\"\n",
    "        \n",
    "        df = agent.execute_dml(sql)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def print_data_summary(name, df, date_col=None):\n",
    "    \"\"\"Print concise summary of loaded data.\"\"\"\n",
    "    if date_col and date_col in df.columns:\n",
    "        date_min = df[date_col].min()\n",
    "        date_max = df[date_col].max()\n",
    "        print(f\"{name}: {len(df)} records | {df.shape[1]} columns | {date_min} to {date_max}\")\n",
    "    else:\n",
    "        print(f\"{name}: {len(df)} records | {df.shape[1]} columns\")\n",
    "\n",
    "def check_missing_values(df, name):\n",
    "    \"\"\"Check for missing values and return summary.\"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(missing[missing > 0])\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"{name}: No missing values\")\n",
    "        return False\n",
    "\n",
    "def validate_columns_exist(df, required_cols, df_name=\"DataFrame\"):\n",
    "    \"\"\"Validate that all required columns exist in DataFrame.\"\"\"\n",
    "    missing_cols = set(required_cols) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"{df_name} missing required columns: {missing_cols}\")\n",
    "    return True\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e6b49",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e9077f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "\n",
      "Economic data: 120 records | 10 columns | 2014-01-31 00:00:00 to 2023-12-31 00:00:00\n",
      "Loan losses: 480 records | 4 columns | 2014-01-31 00:00:00 to 2023-12-31 00:00:00\n",
      "Scenarios: 4 records | 11 columns\n",
      "Capital structure: 3 records | 3 columns\n",
      "Portfolio data: 4 records | 6 columns\n",
      "Projected losses: 16 records | 4 columns\n"
     ]
    }
   ],
   "source": [
    "# Load all data tables using helper function with error handling\n",
    "df_econ = load_table(CONFIG['tables']['economic'], order_by='cal_date')\n",
    "df_losses = load_table(CONFIG['tables']['losses'], order_by='cal_date, portfolio_id')\n",
    "df_scenarios = load_table(CONFIG['tables']['scenarios'], order_by='scenario_name')\n",
    "df_capital = load_table(CONFIG['tables']['capital'])\n",
    "df_portfolio = load_table(CONFIG['tables']['portfolio'], order_by='portfolio_id')\n",
    "df_proj_losses = load_table(CONFIG['tables']['proj_losses'], order_by='scenario_id, portfolio_id')\n",
    "\n",
    "# Print concise summaries\n",
    "print(\"Data loaded successfully:\\n\")\n",
    "print_data_summary(\"Economic data\", df_econ, 'cal_date')\n",
    "print_data_summary(\"Loan losses\", df_losses, 'cal_date')\n",
    "print_data_summary(\"Scenarios\", df_scenarios)\n",
    "print_data_summary(\"Capital structure\", df_capital)\n",
    "print_data_summary(\"Portfolio data\", df_portfolio)\n",
    "print_data_summary(\"Projected losses\", df_proj_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59301178",
   "metadata": {},
   "source": [
    "## 3. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bde28e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "1. MISSING VALUES:\n",
      "Economic data: No missing values\n",
      "Loan losses: No missing values\n",
      "\n",
      "2. DUPLICATE RECORDS:\n",
      "Economic data: 0 duplicates\n",
      "Loan losses: 0 duplicates\n",
      "\n",
      "3. DATE CONTINUITY:\n",
      "Regular monthly intervals\n",
      "\n",
      "4. POTENTIAL OUTLIERS (IQR method):\n",
      "  housing_price_change: 2 outliers\n",
      "  credit_spread_bps: 1 outliers\n",
      "\n",
      "5. PORTFOLIO ID VALIDATION:\n",
      "Portfolio IDs match configuration\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Missing values check\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "check_missing_values(df_econ, \"Economic data\")\n",
    "check_missing_values(df_losses, \"Loan losses\")\n",
    "\n",
    "# 2. Duplicate records check\n",
    "print(\"\\n2. DUPLICATE RECORDS:\")\n",
    "dup_econ = df_econ.duplicated(subset=['cal_date']).sum()\n",
    "dup_losses = df_losses.duplicated(subset=['cal_date', 'portfolio_id']).sum()\n",
    "print(f\"Economic data: {dup_econ} duplicates\")\n",
    "print(f\"Loan losses: {dup_losses} duplicates\")\n",
    "\n",
    "# 3. Date continuity check\n",
    "print(\"\\n3. DATE CONTINUITY:\")\n",
    "df_econ_sorted = df_econ.sort_values('cal_date')\n",
    "df_econ_sorted['cal_date_dt'] = pd.to_datetime(df_econ_sorted['cal_date'])\n",
    "date_diff_days = df_econ_sorted['cal_date_dt'].diff().dt.days\n",
    "# Monthly data should have gaps between 28-31 days\n",
    "if date_diff_days.dropna().between(28, 31).all():\n",
    "    print(\"Regular monthly intervals\")\n",
    "else:\n",
    "    print(\"Warning: Irregular date spacing detected\")\n",
    "    gaps = date_diff_days[~date_diff_days.between(28, 31)]\n",
    "    print(f\"  Unusual gaps: {gaps.value_counts().to_dict()}\")\n",
    "\n",
    "# 4. Outlier detection (using IQR method on all numeric variables)\n",
    "print(\"\\n4. POTENTIAL OUTLIERS (IQR method):\")\n",
    "numeric_cols = df_econ.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = df_econ[col].quantile(0.25)\n",
    "    Q3 = df_econ[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((df_econ[col] < (Q1 - 1.5 * IQR)) | (df_econ[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"  {col}: {outliers} outliers\")\n",
    "\n",
    "# 5. Validate portfolio IDs match configuration\n",
    "print(\"\\n5. PORTFOLIO ID VALIDATION:\")\n",
    "portfolio_ids_in_data = set(df_losses['portfolio_id'].unique())\n",
    "portfolio_ids_in_config = set(CONFIG['portfolio_map'].keys())\n",
    "if portfolio_ids_in_data == portfolio_ids_in_config:\n",
    "    print(\"Portfolio IDs match configuration\")\n",
    "else:\n",
    "    missing = portfolio_ids_in_config - portfolio_ids_in_data\n",
    "    extra = portfolio_ids_in_data - portfolio_ids_in_config\n",
    "    if missing:\n",
    "        print(f\"  Warning: IDs in config but not in data: {missing}\")\n",
    "    if extra:\n",
    "        print(f\"  Warning: IDs in data but not in config: {extra}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc794b",
   "metadata": {},
   "source": [
    "## 4. Clean and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91f697c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and validated:\n",
      "  Date range: 2014-01-31 to 2023-12-31\n",
      "  Total months: 120\n",
      "\n",
      "Portfolios in losses data:\n",
      "portfolio_name\n",
      "Commercial_Loans         120\n",
      "Credit_Cards             120\n",
      "Residential_Mortgages    120\n",
      "Securities_Portfolio     120\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert dates to pandas Timestamp (keep as datetime, not date objects)\n",
    "df_econ['cal_date'] = pd.to_datetime(df_econ['cal_date'])\n",
    "df_losses['cal_date'] = pd.to_datetime(df_losses['cal_date'])\n",
    "\n",
    "# Data already sorted from SQL query - no need to re-sort\n",
    "# Validate sort order is correct\n",
    "assert df_econ['cal_date'].is_monotonic_increasing, \"Economic data dates not in order\"\n",
    "assert df_losses.groupby('portfolio_id')['cal_date'].apply(lambda x: x.is_monotonic_increasing).all(), \\\n",
    "    \"Loan loss dates not in order within portfolios\"\n",
    "\n",
    "# Map portfolio IDs to names\n",
    "df_losses['portfolio_name'] = df_losses['portfolio_id'].map(CONFIG['portfolio_map'])\n",
    "\n",
    "# Validate mapping worked\n",
    "assert df_losses['portfolio_name'].notna().all(), \"Some portfolio IDs could not be mapped to names\"\n",
    "\n",
    "print(\"Data cleaned and validated:\")\n",
    "print(f\"  Date range: {df_econ['cal_date'].min().date()} to {df_econ['cal_date'].max().date()}\")\n",
    "print(f\"  Total months: {len(df_econ)}\")\n",
    "print(f\"\\nPortfolios in losses data:\")\n",
    "print(df_losses['portfolio_name'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3b486",
   "metadata": {},
   "source": [
    "## 5. Create Time Series DataFrames by Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a9bb1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residential_Mortgages: 120 records\n",
      "Commercial_Loans: 120 records\n",
      "Credit_Cards: 120 records\n",
      "Securities_Portfolio: 120 records\n",
      "\n",
      "Created 4 portfolio datasets\n",
      "Each dataset: 9 economic indicators + loss metrics\n",
      "Date range: 2014-01-31 to 2023-12-31\n"
     ]
    }
   ],
   "source": [
    "# Merge economic data with loan losses for each portfolio\n",
    "portfolio_data = {}\n",
    "\n",
    "for portfolio_id, portfolio_name in CONFIG['portfolio_map'].items():\n",
    "    # Filter losses for this portfolio\n",
    "    df_portfolio_losses = df_losses[df_losses['portfolio_id'] == portfolio_id].copy()\n",
    "    \n",
    "    # Merge with economic data\n",
    "    df_merged = df_econ.merge(df_portfolio_losses, on='cal_date', how='inner')\n",
    "    \n",
    "    # Validate merge results\n",
    "    expected_count = CONFIG['expected_months']\n",
    "    actual_count = len(df_merged)\n",
    "    assert actual_count == expected_count, \\\n",
    "        f\"{portfolio_name}: Expected {expected_count} records after merge, got {actual_count}\"\n",
    "    \n",
    "    # Validate no missing values introduced\n",
    "    assert df_merged.isnull().sum().sum() == 0, \\\n",
    "        f\"{portfolio_name}: Missing values found after merge\"\n",
    "    \n",
    "    # Store in dictionary\n",
    "    portfolio_data[portfolio_id] = {\n",
    "        'name': portfolio_name,\n",
    "        'data': df_merged\n",
    "    }\n",
    "    \n",
    "    print(f\"{portfolio_name}: {len(df_merged)} records\")\n",
    "\n",
    "print(f\"\\nCreated {len(portfolio_data)} portfolio datasets\")\n",
    "print(f\"Each dataset: {len([c for c in df_econ.columns if c != 'cal_date'])} economic indicators + loss metrics\")\n",
    "print(f\"Date range: {df_merged['cal_date'].min().date()} to {df_merged['cal_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd1222",
   "metadata": {},
   "source": [
    "## 6. Prepare Features (X) and Target (y) for Each Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2780a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residential_Mortgages:\n",
      "  X shape: (120, 9) (samples, features)\n",
      "  y shape: (120,) | mean: 0.746% | std: 0.220%\n",
      "  Date range: 2014-01-31T00:00:00.000000000 to 2023-12-31T00:00:00.000000000\n",
      "Commercial_Loans:\n",
      "  X shape: (120, 9) (samples, features)\n",
      "  y shape: (120,) | mean: 1.408% | std: 0.235%\n",
      "  Date range: 2014-01-31T00:00:00.000000000 to 2023-12-31T00:00:00.000000000\n",
      "Credit_Cards:\n",
      "  X shape: (120, 9) (samples, features)\n",
      "  y shape: (120,) | mean: 13.236% | std: 6.002%\n",
      "  Date range: 2014-01-31T00:00:00.000000000 to 2023-12-31T00:00:00.000000000\n",
      "Securities_Portfolio:\n",
      "  X shape: (120, 9) (samples, features)\n",
      "  y shape: (120,) | mean: 0.237% | std: 0.128%\n",
      "  Date range: 2014-01-31T00:00:00.000000000 to 2023-12-31T00:00:00.000000000\n",
      "\n",
      "Prepared 4 portfolios for modeling\n",
      "Features: 9 economic indicators | Target: loss_rate_percent\n"
     ]
    }
   ],
   "source": [
    "# Prepare X (features) and y (target) for each portfolio\n",
    "economic_vars = CONFIG['economic_vars']\n",
    "prepared_data = {}\n",
    "\n",
    "for portfolio_id, portfolio_info in portfolio_data.items():\n",
    "    df = portfolio_info['data']\n",
    "    \n",
    "    # Validate required columns exist\n",
    "    validate_columns_exist(df, economic_vars, f\"{portfolio_info['name']} data\")\n",
    "    validate_columns_exist(df, ['loss_rate_percent'], f\"{portfolio_info['name']} data\")\n",
    "    \n",
    "    # Features (economic indicators)\n",
    "    X = df[economic_vars].values\n",
    "    \n",
    "    # Target (loss rate)\n",
    "    y = df['loss_rate_percent'].values\n",
    "    \n",
    "    # Validate target is non-negative\n",
    "    assert (y >= 0).all(), f\"{portfolio_info['name']}: Negative loss rates found\"\n",
    "    \n",
    "    # Dates for reference\n",
    "    dates = df['cal_date'].values\n",
    "    \n",
    "    prepared_data[portfolio_id] = {\n",
    "        'name': portfolio_info['name'],\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'dates': dates,\n",
    "        'feature_names': economic_vars,\n",
    "        'n_samples': len(y)\n",
    "    }\n",
    "    \n",
    "    print(f\"{portfolio_info['name']}:\")\n",
    "    print(f\"  X shape: {X.shape} (samples, features)\")\n",
    "    print(f\"  y shape: {y.shape} | mean: {y.mean():.3f}% | std: {y.std():.3f}%\")\n",
    "    print(f\"  Date range: {dates[0]} to {dates[-1]}\")\n",
    "\n",
    "print(f\"\\nPrepared {len(prepared_data)} portfolios for modeling\")\n",
    "print(f\"Features: {len(economic_vars)} economic indicators | Target: loss_rate_percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llekkjiihn",
   "metadata": {},
   "source": [
    "## 6.1 Multicollinearity Analysis (VIF Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "igwaa3cs9j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multicollinearity Analysis (Variance Inflation Factor):\n",
      "============================================================\n",
      "VIF interpretation: <5 = low, 5-10 = moderate, >10 = high multicollinearity\n",
      "\n",
      "unemployment_rate              VIF: 158.02  [⚠ HIGH]\n",
      "corp_bond_spread_bps           VIF:  85.95  [⚠ HIGH]\n",
      "vix_level                      VIF:  49.50  [⚠ HIGH]\n",
      "treasury_10y_rate              VIF:  41.41  [⚠ HIGH]\n",
      "fed_funds_rate                 VIF:  41.38  [⚠ HIGH]\n",
      "gdp_growth                     VIF:  39.67  [⚠ HIGH]\n",
      "credit_spread_bps              VIF:  31.64  [⚠ HIGH]\n",
      "stock_market_decline           VIF:   2.05  [✓ LOW]\n",
      "housing_price_change           VIF:   1.92  [✓ LOW]\n",
      "\n",
      "============================================================\n",
      "Note: High VIF may cause unstable ARIMAX parameter estimates\n",
      "Consider feature selection or dimensionality reduction if needed\n"
     ]
    }
   ],
   "source": [
    "# Calculate Variance Inflation Factor (VIF) to detect multicollinearity\n",
    "# VIF > 10 indicates high multicollinearity, > 5 moderate\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "print(\"Multicollinearity Analysis (Variance Inflation Factor):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"VIF interpretation: <5 = low, 5-10 = moderate, >10 = high multicollinearity\\n\")\n",
    "\n",
    "# Calculate VIF for each feature using economic data\n",
    "X_vif = df_econ[economic_vars].values\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = economic_vars\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif, i) for i in range(len(economic_vars))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "# Display results\n",
    "for idx, row in vif_data.iterrows():\n",
    "    vif = row['VIF']\n",
    "    if vif > 10:\n",
    "        status = \"⚠ HIGH\"\n",
    "    elif vif > 5:\n",
    "        status = \"! MODERATE\"\n",
    "    else:\n",
    "        status = \"✓ LOW\"\n",
    "    print(f\"{row['Feature']:30s} VIF: {vif:6.2f}  [{status}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Note: High VIF may cause unstable ARIMAX parameter estimates\")\n",
    "print(\"Consider feature selection or dimensionality reduction if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0aa14",
   "metadata": {},
   "source": [
    "## 7. Prepare Scenario Data for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09a51456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economic_Recession_Severe:\n",
      "  gdp_growth: -3.5\n",
      "  unemployment_rate: 11.0\n",
      "  housing_price_change: -20.0\n",
      "  fed_funds_rate: 0.5\n",
      "  treasury_10y_rate: 2.0\n",
      "  credit_spread_bps: 400.0\n",
      "  stock_market_decline: -30.0\n",
      "  vix_level: 60.0\n",
      "  corp_bond_spread_bps: 650.0\n",
      "Interest_Rate_Shock_Moderate:\n",
      "  gdp_growth: 0.5\n",
      "  unemployment_rate: 6.5\n",
      "  housing_price_change: -5.0\n",
      "  fed_funds_rate: 6.0\n",
      "  treasury_10y_rate: 5.5\n",
      "  credit_spread_bps: 250.0\n",
      "  stock_market_decline: -15.0\n",
      "  vix_level: 35.0\n",
      "  corp_bond_spread_bps: 350.0\n",
      "Market_Volatility_Crisis_Severe:\n",
      "  gdp_growth: -1.5\n",
      "  unemployment_rate: 8.5\n",
      "  housing_price_change: -12.0\n",
      "  fed_funds_rate: 2.0\n",
      "  treasury_10y_rate: 3.5\n",
      "  credit_spread_bps: 350.0\n",
      "  stock_market_decline: -45.0\n",
      "  vix_level: 65.0\n",
      "  corp_bond_spread_bps: 750.0\n",
      "Mild_Stress_Baseline:\n",
      "  gdp_growth: 1.0\n",
      "  unemployment_rate: 7.0\n",
      "  housing_price_change: -3.0\n",
      "  fed_funds_rate: 3.5\n",
      "  treasury_10y_rate: 4.0\n",
      "  credit_spread_bps: 200.0\n",
      "  stock_market_decline: -8.0\n",
      "  vix_level: 25.0\n",
      "  corp_bond_spread_bps: 250.0\n",
      "\n",
      "Prepared 4 scenarios for stress testing\n",
      "Each scenario has 9 economic indicators\n"
     ]
    }
   ],
   "source": [
    "# Extract scenario values for each economic variable (using vectorized operations instead of iterrows)\n",
    "scenario_features = {}\n",
    "\n",
    "# Validate all economic variables exist in scenario data\n",
    "validate_columns_exist(df_scenarios, economic_vars, \"Scenario data\")\n",
    "\n",
    "for row in df_scenarios.itertuples():\n",
    "    scenario_name = row.scenario_name\n",
    "    scenario_id = row.scenario_id\n",
    "    \n",
    "    # Extract economic variable values in same order as training data\n",
    "    scenario_values = np.array([getattr(row, var) for var in economic_vars])\n",
    "    \n",
    "    scenario_features[scenario_name] = {\n",
    "        'values': scenario_values,\n",
    "        'scenario_id': scenario_id\n",
    "    }\n",
    "    \n",
    "    print(f\"{scenario_name}:\")\n",
    "    for var, val in zip(economic_vars, scenario_values):\n",
    "        print(f\"  {var}: {val}\")\n",
    "\n",
    "print(f\"\\nPrepared {len(scenario_features)} scenarios for stress testing\")\n",
    "print(f\"Each scenario has {len(economic_vars)} economic indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l3vj0m7dpjn",
   "metadata": {},
   "source": [
    "## 7.1 Validate Scenario Values (Out-of-Distribution Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1oqno8111sjh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario Out-of-Distribution Check:\n",
      "============================================================\n",
      "\n",
      "Economic_Recession_Severe:\n",
      "  ⚠ gdp_growth: -3.50 (historical: [1.55, 4.69])\n",
      "  ⚠ unemployment_rate: 11.00 (historical: [4.42, 8.05])\n",
      "  ⚠ housing_price_change: -20.00 (historical: [-6.69, 7.81])\n",
      "  ! fed_funds_rate: 0.50 (>2σ from historical mean 2.18)\n",
      "  ⚠ credit_spread_bps: 400.00 (historical: [80.00, 349.00])\n",
      "  ⚠ stock_market_decline: -30.00 (historical: [-14.36, 9.48])\n",
      "  ⚠ vix_level: 60.00 (historical: [12.10, 42.80])\n",
      "  ⚠ corp_bond_spread_bps: 650.00 (historical: [174.00, 476.00])\n",
      "\n",
      "Interest_Rate_Shock_Moderate:\n",
      "  ⚠ gdp_growth: 0.50 (historical: [1.55, 4.69])\n",
      "  ! housing_price_change: -5.00 (>2σ from historical mean 1.88)\n",
      "  ⚠ fed_funds_rate: 6.00 (historical: [0.43, 3.78])\n",
      "  ⚠ treasury_10y_rate: 5.50 (historical: [1.50, 4.50])\n",
      "  ⚠ stock_market_decline: -15.00 (historical: [-14.36, 9.48])\n",
      "\n",
      "Market_Volatility_Crisis_Severe:\n",
      "  ⚠ gdp_growth: -1.50 (historical: [1.55, 4.69])\n",
      "  ⚠ unemployment_rate: 8.50 (historical: [4.42, 8.05])\n",
      "  ⚠ housing_price_change: -12.00 (historical: [-6.69, 7.81])\n",
      "  ⚠ credit_spread_bps: 350.00 (historical: [80.00, 349.00])\n",
      "  ⚠ stock_market_decline: -45.00 (historical: [-14.36, 9.48])\n",
      "  ⚠ vix_level: 65.00 (historical: [12.10, 42.80])\n",
      "  ⚠ corp_bond_spread_bps: 750.00 (historical: [174.00, 476.00])\n",
      "\n",
      "Mild_Stress_Baseline:\n",
      "  ⚠ gdp_growth: 1.00 (historical: [1.55, 4.69])\n",
      "  ! housing_price_change: -3.00 (>2σ from historical mean 1.88)\n",
      "\n",
      "============================================================\n",
      "Note: Values outside historical range may produce less reliable predictions\n"
     ]
    }
   ],
   "source": [
    "# Check if scenario values are outside historical ranges (out-of-distribution)\n",
    "print(\"Scenario Out-of-Distribution Check:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scenario_name, scenario_data in scenario_features.items():\n",
    "    scenario_values = scenario_data['values']\n",
    "    out_of_range = []\n",
    "    \n",
    "    print(f\"\\n{scenario_name}:\")\n",
    "    for i, var in enumerate(economic_vars):\n",
    "        scenario_val = scenario_values[i]\n",
    "        hist_min = df_econ[var].min()\n",
    "        hist_max = df_econ[var].max()\n",
    "        hist_mean = df_econ[var].mean()\n",
    "        hist_std = df_econ[var].std()\n",
    "        \n",
    "        # Check if outside historical range\n",
    "        if scenario_val < hist_min or scenario_val > hist_max:\n",
    "            out_of_range.append(var)\n",
    "            print(f\"  ⚠ {var}: {scenario_val:.2f} (historical: [{hist_min:.2f}, {hist_max:.2f}])\")\n",
    "        \n",
    "        # Check if more than 2 std from mean\n",
    "        elif abs(scenario_val - hist_mean) > 2 * hist_std:\n",
    "            print(f\"  ! {var}: {scenario_val:.2f} (>2σ from historical mean {hist_mean:.2f})\")\n",
    "    \n",
    "    if not out_of_range and all(abs(scenario_values[i] - df_econ[var].mean()) <= 2 * df_econ[var].std() \n",
    "                                  for i, var in enumerate(economic_vars)):\n",
    "        print(\"  ✓ All values within historical range and 2σ\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Note: Values outside historical range may produce less reliable predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77624eb6",
   "metadata": {},
   "source": [
    "## 8. Prepare Capital and Portfolio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f9ba845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank Capital Structure:\n",
      "  Tier_1_Capital: $800.0M - Common equity and retained earnings\n",
      "  Tier_2_Capital: $200.0M - Subordinated debt and preferred stock\n",
      "  Total_Assets: $8000.0M - Total risk-weighted assets\n",
      "\n",
      "Portfolio Exposures:\n",
      "  Residential_Mortgages: $2000.0M (baseline loss: 0.50%)\n",
      "  Commercial_Loans: $1500.0M (baseline loss: 1.20%)\n",
      "  Credit_Cards: $800.0M (baseline loss: 3.80%)\n",
      "  Securities_Portfolio: $700.0M (baseline loss: 0.10%)\n",
      "\n",
      "Capital Metrics:\n",
      "  Total Portfolio Exposure: $5000.0M\n",
      "  Tier 1 Capital: $800.0M\n",
      "  Total Regulatory Capital (Tier 1 + Tier 2): $1000.0M\n",
      "  Risk-Weighted Assets: $8000.0M\n",
      "  Tier 1 Capital Ratio: 10.00%\n",
      "  Total Capital Ratio: 12.50%\n",
      "\n",
      "Baseline projected losses loaded for comparison with ARIMAX predictions\n"
     ]
    }
   ],
   "source": [
    "# Prepare capital structure data\n",
    "capital_info = {}\n",
    "for row in df_capital.itertuples():\n",
    "    capital_info[row.capital_type] = {\n",
    "        'amount_millions': row.amount_millions,\n",
    "        'description': row.description\n",
    "    }\n",
    "\n",
    "print(\"Bank Capital Structure:\")\n",
    "for capital_type, info in capital_info.items():\n",
    "    print(f\"  {capital_type}: ${info['amount_millions']:.1f}M - {info['description']}\")\n",
    "\n",
    "# Calculate capital ratios correctly\n",
    "# Tier 1 + Tier 2 = Total Regulatory Capital\n",
    "# Total Assets = Risk-Weighted Assets (RWA)\n",
    "tier1_capital = capital_info.get('Tier_1_Capital', {}).get('amount_millions', 0)\n",
    "tier2_capital = capital_info.get('Tier_2_Capital', {}).get('amount_millions', 0)\n",
    "total_regulatory_capital = tier1_capital + tier2_capital\n",
    "risk_weighted_assets = capital_info.get('Total_Assets', {}).get('amount_millions', 0)\n",
    "\n",
    "# Prepare portfolio exposure data\n",
    "portfolio_exposure = {}\n",
    "total_exposure = 0\n",
    "\n",
    "for row in df_portfolio.itertuples():\n",
    "    portfolio_id = row.portfolio_id\n",
    "    portfolio_exposure[portfolio_id] = {\n",
    "        'name': CONFIG['portfolio_map'][portfolio_id],\n",
    "        'balance_millions': row.outstanding_balance_millions,\n",
    "        'avg_interest_rate': row.avg_interest_rate,\n",
    "        'baseline_loss_rate': row.baseline_loss_rate,\n",
    "        'duration_years': row.duration_years\n",
    "    }\n",
    "    total_exposure += row.outstanding_balance_millions\n",
    "\n",
    "print(\"\\nPortfolio Exposures:\")\n",
    "for pid, info in portfolio_exposure.items():\n",
    "    print(f\"  {info['name']}: ${info['balance_millions']:.1f}M (baseline loss: {info['baseline_loss_rate']:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCapital Metrics:\")\n",
    "print(f\"  Total Portfolio Exposure: ${total_exposure:.1f}M\")\n",
    "print(f\"  Tier 1 Capital: ${tier1_capital:.1f}M\")\n",
    "print(f\"  Total Regulatory Capital (Tier 1 + Tier 2): ${total_regulatory_capital:.1f}M\")\n",
    "print(f\"  Risk-Weighted Assets: ${risk_weighted_assets:.1f}M\")\n",
    "print(f\"  Tier 1 Capital Ratio: {(tier1_capital / risk_weighted_assets * 100):.2f}%\")\n",
    "print(f\"  Total Capital Ratio: {(total_regulatory_capital / risk_weighted_assets * 100):.2f}%\")\n",
    "\n",
    "# Prepare projected losses for comparison\n",
    "projected_losses_by_scenario = {}\n",
    "\n",
    "for scenario_name in df_scenarios['scenario_name'].unique():\n",
    "    scenario_id = df_scenarios[df_scenarios['scenario_name'] == scenario_name]['scenario_id'].values[0]\n",
    "    scenario_projections = df_proj_losses[df_proj_losses['scenario_id'] == scenario_id]\n",
    "    \n",
    "    projected_losses_by_scenario[scenario_name] = {}\n",
    "    for row in scenario_projections.itertuples():\n",
    "        portfolio_id = row.portfolio_id\n",
    "        projected_losses_by_scenario[scenario_name][portfolio_id] = {\n",
    "            'projected_loss_rate': row.projected_loss_rate_percent,\n",
    "            'projected_loss_amount': row.projected_loss_amount_millions\n",
    "        }\n",
    "\n",
    "print(\"\\nBaseline projected losses loaded for comparison with ARIMAX predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ceb324",
   "metadata": {},
   "source": [
    "## 9. Summary of Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d1df303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA PREPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "PORTFOLIO DATA (for training):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Residential_Mortgages:\n",
      "  Samples: 120 | Features: 9\n",
      "  Loss rate - mean: 0.746% | std: 0.220%\n",
      "  Exposure: $2000.0M | Baseline: 0.50%\n",
      "\n",
      "Commercial_Loans:\n",
      "  Samples: 120 | Features: 9\n",
      "  Loss rate - mean: 1.408% | std: 0.235%\n",
      "  Exposure: $1500.0M | Baseline: 1.20%\n",
      "\n",
      "Credit_Cards:\n",
      "  Samples: 120 | Features: 9\n",
      "  Loss rate - mean: 13.236% | std: 6.002%\n",
      "  Exposure: $800.0M | Baseline: 3.80%\n",
      "\n",
      "Securities_Portfolio:\n",
      "  Samples: 120 | Features: 9\n",
      "  Loss rate - mean: 0.237% | std: 0.128%\n",
      "  Exposure: $700.0M | Baseline: 0.10%\n",
      "\n",
      "\n",
      "ECONOMIC FEATURES (9 total):\n",
      "------------------------------------------------------------\n",
      " 1. gdp_growth\n",
      " 2. unemployment_rate\n",
      " 3. housing_price_change\n",
      " 4. fed_funds_rate\n",
      " 5. treasury_10y_rate\n",
      " 6. credit_spread_bps\n",
      " 7. stock_market_decline\n",
      " 8. vix_level\n",
      " 9. corp_bond_spread_bps\n",
      "\n",
      "\n",
      "STRESS TEST SCENARIOS (4 total):\n",
      "------------------------------------------------------------\n",
      "  - Economic_Recession_Severe\n",
      "  - Interest_Rate_Shock_Moderate\n",
      "  - Market_Volatility_Crisis_Severe\n",
      "  - Mild_Stress_Baseline\n",
      "\n",
      "\n",
      "BANK CAPITAL POSITION:\n",
      "------------------------------------------------------------\n",
      "  Total Portfolio Exposure: $5000.0M\n",
      "  Tier 1 Capital: $800.0M\n",
      "  Total Regulatory Capital: $1000.0M\n",
      "  Risk-Weighted Assets: $8000.0M\n",
      "  Tier 1 Capital Ratio: 10.00%\n",
      "  Total Capital Ratio: 12.50%\n",
      "\n",
      "\n",
      "DATA QUALITY CHECKS COMPLETED:\n",
      "------------------------------------------------------------\n",
      "  - All 6 data tables loaded with error handling\n",
      "  - No missing values detected\n",
      "  - No duplicate records found\n",
      "  - Date continuity validated (monthly frequency)\n",
      "  - Portfolio IDs validated against configuration\n",
      "  - Merge operations validated (no data loss)\n",
      "  - Feature columns exist and validated\n",
      "  - Target variables non-negative\n",
      "  - Multicollinearity assessed (VIF scores)\n",
      "  - Scenario values checked for out-of-distribution\n",
      "  - Capital ratios calculated correctly\n",
      "\n",
      "============================================================\n",
      "Ready to proceed to modeling.ipynb!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPARATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPORTFOLIO DATA (for training):\")\n",
    "print(\"-\" * 60)\n",
    "for portfolio_id, data in prepared_data.items():\n",
    "    exposure = portfolio_exposure[portfolio_id]['balance_millions']\n",
    "    baseline_loss = portfolio_exposure[portfolio_id]['baseline_loss_rate']\n",
    "    print(f\"\\n{data['name']}:\")\n",
    "    print(f\"  Samples: {data['n_samples']} | Features: {len(data['feature_names'])}\")\n",
    "    print(f\"  Loss rate - mean: {data['y'].mean():.3f}% | std: {data['y'].std():.3f}%\")\n",
    "    print(f\"  Exposure: ${exposure:.1f}M | Baseline: {baseline_loss:.2f}%\")\n",
    "\n",
    "print(\"\\n\\nECONOMIC FEATURES ({} total):\".format(len(economic_vars)))\n",
    "print(\"-\" * 60)\n",
    "for i, var in enumerate(economic_vars, 1):\n",
    "    print(f\"{i:2d}. {var}\")\n",
    "\n",
    "print(\"\\n\\nSTRESS TEST SCENARIOS ({} total):\".format(len(scenario_features)))\n",
    "print(\"-\" * 60)\n",
    "for scenario_name in scenario_features.keys():\n",
    "    print(f\"  - {scenario_name}\")\n",
    "\n",
    "print(\"\\n\\nBANK CAPITAL POSITION:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total Portfolio Exposure: ${total_exposure:.1f}M\")\n",
    "print(f\"  Tier 1 Capital: ${tier1_capital:.1f}M\")\n",
    "print(f\"  Total Regulatory Capital: ${total_regulatory_capital:.1f}M\")\n",
    "print(f\"  Risk-Weighted Assets: ${risk_weighted_assets:.1f}M\")\n",
    "print(f\"  Tier 1 Capital Ratio: {(tier1_capital / risk_weighted_assets * 100):.2f}%\")\n",
    "print(f\"  Total Capital Ratio: {(total_regulatory_capital / risk_weighted_assets * 100):.2f}%\")\n",
    "\n",
    "print(\"\\n\\nDATA QUALITY CHECKS COMPLETED:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  - All 6 data tables loaded with error handling\")\n",
    "print(\"  - No missing values detected\")\n",
    "print(\"  - No duplicate records found\")\n",
    "print(\"  - Date continuity validated (monthly frequency)\")\n",
    "print(\"  - Portfolio IDs validated against configuration\")\n",
    "print(\"  - Merge operations validated (no data loss)\")\n",
    "print(\"  - Feature columns exist and validated\")\n",
    "print(\"  - Target variables non-negative\")\n",
    "print(\"  - Multicollinearity assessed (VIF scores)\")\n",
    "print(\"  - Scenario values checked for out-of-distribution\")\n",
    "print(\"  - Capital ratios calculated correctly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Ready to proceed to modeling.ipynb!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads_5984",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
